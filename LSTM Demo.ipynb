{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Sentiment Analysis DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vec(text_file, save_file, user_dicts=None, drop=None, size=128, **kwargs):\n",
    "    \"\"\"\n",
    "    Train w2v model based on th etext file and store to the local system\n",
    "    \"\"\"\n",
    "    if not drop:\n",
    "        drop = [\" \"]\n",
    "        \n",
    "    if user_dicts:\n",
    "        for ud in user_dicts:\n",
    "            jieba.load_userdict(ud)\n",
    "    \n",
    "    if isinstance(text_file, str):\n",
    "        with open(text_file, \"rb\") as f:\n",
    "            doc = f.readlines()\n",
    "    else:\n",
    "        doc = text_file\n",
    "    \n",
    "    sentences = []\n",
    "    for line in doc:\n",
    "        words = jieba.lcut(line.strip())\n",
    "        words = [w for w in words if w not in drop]\n",
    "        sentences.append(words)\n",
    "            \n",
    "    model = Word2Vec(sentences, size=size, **kwargs)\n",
    "    model.wv.save_word2vec_format(save_file, binary=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_w2v(w2v_file):\n",
    "    \"\"\"\n",
    "    load w2v file and return gensim.models.word2vec.Word2Vec object\n",
    "    \"\"\"\n",
    "    \n",
    "    return KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_w2ix(w2v_model):\n",
    "    \"\"\"\n",
    "    Create a dictionary\n",
    "    \"\"\"\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(w2v_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2ix = {v: k + 1 for k, v in gensim_dict.items()}\n",
    "    \n",
    "    return w2ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ix_vec(sents: List[List[str]], w2ix):\n",
    "    \"\"\"\n",
    "    Transfer a file to index array\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sen in sents:\n",
    "        new_sen = []\n",
    "        for word in sen:\n",
    "            try:\n",
    "                new_sen.append(w2ix[word])\n",
    "            except:\n",
    "                new_sen.append(0)\n",
    "        new_sentences.append(np.array(new_sen))\n",
    "\n",
    "    return np.array(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_w2ix_weight(index_dic, w2v_model):\n",
    "    \"\"\"\n",
    "    Generate weights from w2v corresponding to the dictionary\n",
    "    \"\"\"\n",
    "    weights = np.zeros((len(index_dic)+1, w2v_model.vector_size))\n",
    "    for w, index in index_dic.items():\n",
    "        weights[index, :] = w2v_model[w]\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer of LSTM\n",
    "def train_lstm(embedding_weights, x_train, y_train, x_test, y_test, **kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    print ('Creating a model...')\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim = 128,\n",
    "                        input_dim = W2IX_DIM,\n",
    "                        mask_zero = True,\n",
    "                        weights = [embedding_weights],\n",
    "                        input_length = INPUT_LEN,\n",
    "                       ))  # Adding Input Length\n",
    "    model.add(LSTM(input_dim=128, \n",
    "                   output_dim = kwargs.get('lstm_out_dim', 64), \n",
    "                   activation = kwargs.get('lstm_actv', 'tanh'),\n",
    "                   dropout=kwargs.get('lstm_drop_out', .2)))\n",
    "    model.add(Dropout(kwargs.get('drop_out', .3)))\n",
    "    model.add(Dense(output_dim=N_CLASS, activation=kwargs.get('dens_actv', 'softmax')))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    print ('Compiling...)\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    print (\"Training...\")\n",
    "    model.fit(x_train, y_train, batch_size = BATCH_SIZE, nb_epoch = EPOCH, validation_data = (x_test, y_test))\n",
    "\n",
    "    print (\"Evaluating...\")\n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size = BATCH_SIZE)\n",
    "    print ('Test score: %.3f' % score)\n",
    "    print ('Test accuracy: %.3f' % acc)\n",
    "    return model, kwargs.get('dens_actv', 'softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Demo 1 Sentiment analysis of online shopping evaluation (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import copy\n",
    "\n",
    "with open('./data/comments/pos.txt','r', encoding='utf-8') as f:\n",
    "    pos = f.readlines()\n",
    "    label_pos = [1]*len(pos)\n",
    "with open('./data/comments/neg.txt','r', encoding='utf-8') as f:\n",
    "    neg = f.readlines()\n",
    "    label_neg = [0]*len(neg)\n",
    "    \n",
    "all_txt = copy.copy(pos)\n",
    "all_txt.extend(neg)\n",
    "\n",
    "all_lable = copy.copy(label_pos)\n",
    "all_lable.extend(label_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainw2v\n",
    "# w2v = build_word_vec(all_txt, \"data/comments/comments_w2v.bin\", min_count=5, window=5)\n",
    "w2v = load_w2v(\"data/comments/comments_w2v.bin\")\n",
    "w2ix = gen_w2ix(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "N_CLASS = 2\n",
    "W2IX_DIM = len(w2ix) + 1\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t15843 \n",
      "Test set: \t\t5281\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "comments = get_ix_vec(all_txt, w2ix)\n",
    "weights = gen_w2ix_weight(w2ix, w2v.wv)\n",
    "\n",
    "INPUT_LEN = max([len(l) for l in comments])\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(comments, np.array(all_lable))\n",
    "train_x = sequence.pad_sequences(train_x, INPUT_LEN)\n",
    "test_x = sequence.pad_sequences(test_x, INPUT_LEN)\n",
    "train_y = to_categorical(train_y,num_classes = N_CLASS)\n",
    "test_y = to_categorical(test_y,num_classes = N_CLASS)\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(len(train_x)), \"\\nTest set: \\t\\t{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fitted = train_lstm(weights, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试效果\n",
    "# pos 热水器挺好的，比实体店优惠多了，关键是好用，发货速度 物流速度都快，还有赠品\n",
    "# neg 总的感觉前言不搭后语，浪费了银子，呜呜\n",
    "t_txt = \"总的感觉前言不搭后语，浪费了银子，呜呜\"\n",
    "test = sequence.pad_sequences([get_ix_vec([t_txt], w2ix)[0]], 45)\n",
    "fitted.predict_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## DEMO 2 Sentiment Analysis of financial news headlines (Three categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train w2v\n",
    "sentences = []\n",
    "for i in [1,2,3]:\n",
    "    with open('./data/news/doc_p'+str(i)+'.txt','r',encoding = 'utf-8') as f:\n",
    "        doc = f.readlines()\n",
    "        doc = [d for d in doc if len(d) > 20]\n",
    "    lines = []\n",
    "    for l in doc:\n",
    "        words = jieba.lcut(l.strip())\n",
    "        words = [w for w in words if w != \" \"]\n",
    "        lines.append(words)\n",
    "    sentences.extend(lines)\n",
    "    del lines, doc\n",
    "\n",
    "model = Word2Vec(sentences, size=128, min_count=10, window=5)\n",
    "model.wv.save_word2vec_format(\"./data/news/news_w2v.bin\", binary=True)\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod title and article\n",
    "with open('./data/news/titles_sign+3.txt','r',encoding = 'utf-8') as f:\n",
    "    titles = f.read()\n",
    "with open('./data/news/sentiments_sign+3.txt','r',encoding = 'utf-8') as f:\n",
    "    sentiments = f.read()\n",
    "\n",
    "from string import punctuation\n",
    "symbols = '！；：？【】★■●↑“”，。、~@#￥%《》……&*（）0123456789' + punctuation\n",
    "titles = ''.join([c for c in titles  if c not in symbols])\n",
    "\n",
    "all_txt = titles.split('\\t')[:-1]    \n",
    "all_lable = sentiments.split(\"\\t\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = load_w2v(\"data/news/news_w2v.bin\")\n",
    "w2ix = gen_w2ix(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "N_CLASS = 3\n",
    "W2IX_DIM = len(w2ix) + 1\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t1450 \n",
      "Test set: \t\t484\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "titles = get_ix_vec(all_txt, w2ix)\n",
    "weights = gen_w2ix_weight(w2ix, w2v.wv)\n",
    "\n",
    "INPUT_LEN = max([len(l) for l in titles])\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(titles, np.array(all_lable))\n",
    "train_x = sequence.pad_sequences(train_x, INPUT_LEN)\n",
    "test_x = sequence.pad_sequences(test_x, INPUT_LEN)\n",
    "train_y = to_categorical(train_y,num_classes = N_CLASS)\n",
    "test_y = to_categorical(test_y,num_classes = N_CLASS)\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(len(train_x)), \"\\nTest set: \\t\\t{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fitted = train_lstm(weights, train_x, train_y, test_x, test_y, dens_actv='sigmoid')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the result\n",
    "# pos 凯撒旅游与中国光大银行深化合作 打开全渠道发展空间\n",
    "# neg 小康股份收购美国电池公司，遭上交所问询\n",
    "# neu 龙韵股份05月29日资金揭秘\n",
    "t_txt = \"贵州茅台已经“失控”国家队都拯救不了\"\n",
    "test = sequence.pad_sequences([get_ix_vec([t_txt], w2ix)[0]], INPUT_LEN)\n",
    "fitted.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
